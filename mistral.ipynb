{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-xmpmhnax\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-xmpmhnax\n",
      "  fatal: unable to access 'https://github.com/huggingface/transformers.git/': Could not resolve host: github.com\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[4;32mhttps://github.com/huggingface/transformers.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/\u001b[0m\u001b[32mpip-req-build-xmpmhnax\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[4;32mhttps://github.com/huggingface/transformers.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/\u001b[0m\u001b[32mpip-req-build-xmpmhnax\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# pip install xformers\n",
    "# pip install huggingface_hub\n",
    "# pip install auto-gptq\n",
    "# pip install optimum\n",
    "# pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c78e8f73524cfda1f6bf253b257536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 5.77 GiB of which 29.31 MiB is free. Including non-PyTorch memory, this process has 4.67 GiB memory in use. Of the allocated memory 4.43 GiB is allocated by PyTorch, and 128.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/hardk/AMNESIA/Megathon/mistral.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hardk/AMNESIA/Megathon/mistral.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model_name_or_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTheBloke/Mistral-7B-Instruct-v0.1-GPTQ\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hardk/AMNESIA/Megathon/mistral.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# To use a different branch, change revision\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hardk/AMNESIA/Megathon/mistral.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# For example: revision=\"gptq-4bit-32g-actorder_True\"\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hardk/AMNESIA/Megathon/mistral.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_name_or_path,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hardk/AMNESIA/Megathon/mistral.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                              device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hardk/AMNESIA/Megathon/mistral.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                              torchscript\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hardk/AMNESIA/Megathon/mistral.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                                              trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hardk/AMNESIA/Megathon/mistral.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                                              revision\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hardk/AMNESIA/Megathon/mistral.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name_or_path, use_fast\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hardk/AMNESIA/Megathon/mistral.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m model\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:565\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    564\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 565\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    566\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3377\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3375\u001b[0m     model\u001b[39m.\u001b[39m_is_quantized_training_enabled \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   3376\u001b[0m \u001b[39mif\u001b[39;00m quantization_method_from_config \u001b[39m==\u001b[39m QuantizationMethod\u001b[39m.\u001b[39mGPTQ:\n\u001b[0;32m-> 3377\u001b[0m     model \u001b[39m=\u001b[39m quantizer\u001b[39m.\u001b[39;49mpost_init_model(model)\n\u001b[1;32m   3379\u001b[0m \u001b[39mif\u001b[39;00m _adapter_model_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3380\u001b[0m     model\u001b[39m.\u001b[39mload_adapter(\n\u001b[1;32m   3381\u001b[0m         _adapter_model_path,\n\u001b[1;32m   3382\u001b[0m         adapter_name\u001b[39m=\u001b[39madapter_name,\n\u001b[1;32m   3383\u001b[0m         token\u001b[39m=\u001b[39mtoken,\n\u001b[1;32m   3384\u001b[0m         adapter_kwargs\u001b[39m=\u001b[39madapter_kwargs,\n\u001b[1;32m   3385\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/optimum/gptq/quantizer.py:492\u001b[0m, in \u001b[0;36mGPTQQuantizer.post_init_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    490\u001b[0m model\u001b[39m.\u001b[39mquantize_config \u001b[39m=\u001b[39m StoreAttr()\n\u001b[1;32m    491\u001b[0m model\u001b[39m.\u001b[39mquantize_config\u001b[39m.\u001b[39mdesc_act \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdesc_act\n\u001b[0;32m--> 492\u001b[0m model \u001b[39m=\u001b[39m autogptq_post_init(model, use_act_order\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdesc_act)\n\u001b[1;32m    493\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdesc_act \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable_exllama \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_input_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m     model \u001b[39m=\u001b[39m exllama_set_max_input_length(model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_input_length)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/auto_gptq/modeling/_utils.py:249\u001b[0m, in \u001b[0;36mautogptq_post_init\u001b[0;34m(model, use_act_order, max_input_length)\u001b[0m\n\u001b[1;32m    242\u001b[0m     max_input_len \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    244\u001b[0m \u001b[39mfor\u001b[39;00m device, buffers_size \u001b[39min\u001b[39;00m device_to_buffers_size\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    245\u001b[0m     \u001b[39m# The temp_state buffer is required to reorder X in the act-order case.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[39m# The temp_dq buffer is required to dequantize weights when using cuBLAS, typically for the prefill.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     device_to_buffers[device] \u001b[39m=\u001b[39m {\n\u001b[1;32m    248\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtemp_state\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mzeros((max_input_len, buffers_size[\u001b[39m\"\u001b[39m\u001b[39mmax_inner_outer_dim\u001b[39m\u001b[39m\"\u001b[39m]), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat16, device\u001b[39m=\u001b[39mdevice),\n\u001b[0;32m--> 249\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtemp_dq\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39;49mzeros((\u001b[39m1\u001b[39;49m, buffers_size[\u001b[39m\"\u001b[39;49m\u001b[39mmax_dq_buffer_size\u001b[39;49m\u001b[39m\"\u001b[39;49m]), dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat16, device\u001b[39m=\u001b[39;49mdevice),\n\u001b[1;32m    250\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmax_dq_buffer_size\u001b[39m\u001b[39m\"\u001b[39m: buffers_size[\u001b[39m\"\u001b[39m\u001b[39mmax_dq_buffer_size\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    251\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmax_inner_outer_dim\u001b[39m\u001b[39m\"\u001b[39m: buffers_size[\u001b[39m\"\u001b[39m\u001b[39mmax_inner_outer_dim\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    252\u001b[0m     }\n\u001b[1;32m    254\u001b[0m \u001b[39m# Buffers need to be persistent to avoid any bug.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m model\u001b[39m.\u001b[39mdevice_to_buffers \u001b[39m=\u001b[39m device_to_buffers\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 5.77 GiB of which 29.31 MiB is free. Including non-PyTorch memory, this process has 4.67 GiB memory in use. Of the allocated memory 4.43 GiB is allocated by PyTorch, and 128.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             torchscript=True,\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hardk/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s>[INST] Given a dictionary with 'context' and 'question', answer the 'question' based on 'context' ---- [Context: 'a girl name sara loves to eat chocolate'; Question: Who loves to eat broccoli?] [/INST]\n",
      "Answer: Sara does not love to eat broccoli.</s>\n",
      "*** Pipeline:\n",
      "<s>[INST] Given a dictionary with 'context' and 'question', answer the 'question' based on 'context' ---- [Context: 'a girl name sara loves to eat chocolate'; Question: Who loves to eat broccoli?] [/INST]\n",
      "Answer: Sara does not love to eat broccoli.\n"
     ]
    }
   ],
   "source": [
    "RAG_context = \"\"\n",
    "User_query = \"\"\n",
    "prompt = f\"Given a dictionary with 'context' and 'question', answer the 'question' based on 'context' ---- [Context: '{RAG_context}'; Question: '{User_query}']\"\n",
    "prompt_template=f'''<s>[INST] {prompt} [/INST]\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
