{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##### Install requirements"
      ],
      "metadata": {
        "id": "TJOUtJlH4NLb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6zm3A-c3Imi"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install xformers\n",
        "!pip install huggingface_hub\n",
        "!pip install auto-gptq\n",
        "!pip install optimum\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Login to HuggingFace using your access token"
      ],
      "metadata": {
        "id": "ysm1ZnOT4Q9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "MFMCm3Rq3Oip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Prepare the model\n"
      ],
      "metadata": {
        "id": "ODRioDaA4WFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
        "# To use a different branch, change revision\n",
        "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map=\"auto\",\n",
        "                                             torchscript=True,\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"main\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
      ],
      "metadata": {
        "id": "-E6eizFX3RJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Verify inference"
      ],
      "metadata": {
        "id": "efHkBVlV4hF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.cuda()\n",
        "\n",
        "prompt = \"Given a dictionary with 'context' and 'question', answer the 'question' based on 'context' ---- [Context: 'a boy name astitva loves to eat dark chocolate'; Question: Who loves to eat dark chocolate?]\"\n",
        "prompt_template=f'''<s>[INST] {prompt} [/INST]\n",
        "'''\n",
        "\n",
        "print(\"\\n\\n*** Generate:\")\n",
        "\n",
        "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "id": "3_HnMRtt3jNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Trace the model to get the executable that is optimized using just-in-time compilation"
      ],
      "metadata": {
        "id": "5p7EAAiZ3sK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "traced_model = torch.jit.trace(model, input_ids)"
      ],
      "metadata": {
        "id": "I32DNjAz3mqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimize and generate '.ptl'"
      ],
      "metadata": {
        "id": "Tr0eMBLE4FzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "optimized_traced_model = optimize_for_mobile(traced_model)\n",
        "optimized_traced_model._save_for_lite_interpreter(\"mistral7B_quantized.ptl\")"
      ],
      "metadata": {
        "id": "iYT0wYUa3qNC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
